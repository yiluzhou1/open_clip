{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1=torch.Size([1, 77])\n",
      "text_features_ViT-B-32: torch.Size([1, 512])\n",
      "text2=torch.Size([1, 77])\n",
      "text_features_ViT-L-14: torch.Size([1, 768])\n",
      "text3=torch.Size([1, 77])\n",
      "text_features_ViT-B-32: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import numpy as np\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=\"cpu\", jit=False)\n",
    "text = open_clip.tokenize([\"MRI\"])\n",
    "print(f'text1={text.shape}')\n",
    "text_features1 = model.encode_text(text)\n",
    "print(f'text_features_ViT-B-32: {text_features1.shape}') #torch.Size([1, 512]): tensor([[ 1.3201e-01, -1.7446e-01, -3.3043e-01, -3.1494e-01, -3.9451e-02,\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai', device=\"cpu\", jit=False)\n",
    "text = open_clip.tokenize([\"MRI\"])\n",
    "print(f'text2={text.shape}')\n",
    "text_features1 = model.encode_text(text)\n",
    "print(f'text_features_ViT-L-14: {text_features1.shape}') #torch.Size([1, 768]): tensor([[-1.9208e-01,  2.1010e-02,  1.9233e-01, -1.8018e-01, -9.7014e-02,\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2b_s13b_b90k', device=\"cpu\", jit=False)\n",
    "text = open_clip.tokenize([\"MRI\"])\n",
    "print(f'text3={text.shape}')\n",
    "text_features1 = model.encode_text(text)\n",
    "print(f'text_features_ViT-B-32: {text_features1.shape}') #torch.Size([1, 768]): tensor([[ 1.8017e-02,  4.3765e-02,  7.0854e-03,  6.3915e-03,  1.1728e-02,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_onnx.shape: (3, 77)\n",
      "[CLIP ONNX] Start convert visual model\n",
      "[CLIP ONNX] Start check visual model\n",
      "[CLIP ONNX] Start convert textual model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5856: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIP ONNX] Start check textual model\n",
      "[CLIP ONNX] Models converts successfully\n",
      "text_features_onnx.shape: (3, 512)\n",
      "Label probs: [[0.01968996 0.93325    0.04706011]]\n",
      "test:test\n",
      "text_features1_openclip.shape: torch.Size([3, 512])\n",
      "Label probs: tensor([[0.0197, 0.9333, 0.0471]])\n"
     ]
    }
   ],
   "source": [
    "# Load standard CLIP model, image, text on cpu\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from clip_onnx import clip_onnx, attention\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=\"cpu\", jit=False)\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "# model.eval()\n",
    "# batch first\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# batch first\n",
    "# tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "# text = tokenizer([\"MRI\", \"Xray\", \"CT\"])\n",
    "text = open_clip.tokenize([\"MRI\", \"Xray\", \"CT\"]) #\n",
    "text_onnx = text.detach().cpu().numpy().astype(np.int64)\n",
    "print(f'text_onnx.shape: {text_onnx.shape}')\n",
    "\n",
    "#Create CLIP-ONNX object to convert model to onnx\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "visual_path = \"clip_visual.onnx\"\n",
    "textual_path = \"clip_textual.onnx\"\n",
    "\n",
    "onnx_model = clip_onnx(model, visual_path=visual_path, textual_path=textual_path)\n",
    "onnx_model.convert2onnx(image, text, verbose=True)\n",
    "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode\n",
    "\n",
    "#Use for standard CLIP API. Batch inference\n",
    "image_features = onnx_model.encode_image(image_onnx)\n",
    "# print(f'image_features: {image_features}')\n",
    "text_features = onnx_model.encode_text(text_onnx)\n",
    "print(f'text_features_onnx.shape: {text_features.shape}')\n",
    "logits_per_image, logits_per_text = onnx_model(image_onnx, text_onnx)\n",
    "text_probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "# image_features /= np.linalg.norm(image_features, axis=-1, keepdims=True)\n",
    "# text_features /= np.linalg.norm(text_features, axis=-1, keepdims=True)\n",
    "# text_probs = softmax(100.0 * image_features @ text_features.T)\n",
    "print(\"Label probs:\", text_probs)  # prints: [[0.27080908 0.44203502 0.28715587]]\n",
    "\n",
    "# with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "image_features = model.encode_image(image)\n",
    "# print(f'image_features: {image_features}')\n",
    "text_features1 = model.encode_text(text)\n",
    "print(f'text_features1_openclip.shape: {text_features1.shape}')\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features1 /= text_features1.norm(dim=-1, keepdim=True)\n",
    "text_probs = (100.0 * image_features @ text_features1.T).softmax(dim=-1)\n",
    "print(\"Label probs:\", text_probs)  # prints: tensor([[0.0197, 0.9333, 0.0471]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: _build_text_tower______________________\n",
      "TODO: embed_dim = 768\n",
      "TODO: text_cfg = CLIPTextCfg(context_length=76, vocab_size=49408, width=768, heads=12, layers=12, ls_init_value=None, hf_model_name=None, hf_tokenizer_name=None, hf_model_pretrained=True, proj='mlp', pooler_type='mean_pooler', embed_cls=True, pad_id=0, output_tokens=True)\n",
      "TODO: quick_gelu = False\n",
      "TODO: cast_dtype = None\n",
      "text_onnx.shape: (3, 77)\n",
      "text_features1_openclip.shape: torch.Size([3, 768])\n",
      "Label probs: tensor([[1.5892e-01, 8.4095e-01, 1.3302e-04]])\n"
     ]
    }
   ],
   "source": [
    "# Load standard OPENCLIP Coca model, image, text on cpu\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from clip_onnx import clip_onnx\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2b_s13b_b90k', device=\"cpu\", jit=False)\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "model.eval()\n",
    "# batch first\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# batch first\n",
    "# tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "# text = tokenizer([\"MRI\", \"Xray\", \"CT\"])\n",
    "text = open_clip.tokenize([\"MRI\", \"Xray\", \"CT\"]) #\n",
    "text_onnx = text.detach().cpu().numpy().astype(np.int64)\n",
    "print(f'text_onnx.shape: {text_onnx.shape}')\n",
    "\n",
    "#Create CLIP-ONNX object to convert model to onnx\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "visual_path = \"clip_visual.onnx\"\n",
    "textual_path = \"clip_textual.onnx\"\n",
    "\n",
    "# onnx_model = clip_onnx(model, visual_path=visual_path, textual_path=textual_path)\n",
    "# onnx_model.convert2onnx(image, text, verbose=True)\n",
    "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "# onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode\n",
    "\n",
    "#Use for standard CLIP API. Batch inference\n",
    "# image_features = onnx_model.encode_image(image_onnx)\n",
    "# text_features = onnx_model.encode_text(text_onnx)\n",
    "# print(f'text_features_onnx.shape: {text_features.shape}')\n",
    "# logits_per_image, logits_per_text = onnx_model(image_onnx, text_onnx)\n",
    "# text_probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "# # image_features /= np.linalg.norm(image_features, axis=-1, keepdims=True)\n",
    "# # text_features /= np.linalg.norm(text_features, axis=-1, keepdims=True)\n",
    "# # text_probs = softmax(100.0 * image_features @ text_features.T)\n",
    "# print(\"Label probs:\", text_probs)  # prints: [[0.27080908 0.44203502 0.28715587]]\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    # print(f'image_features: {image_features}')\n",
    "    text_features1 = model.encode_text(text)\n",
    "    print(f'text_features1_openclip.shape: {text_features1.shape}')\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features1 /= text_features1.norm(dim=-1, keepdim=True)\n",
    "    text_probs = (100.0 * image_features @ text_features1.T).softmax(dim=-1)\n",
    "    print(\"Label probs:\", text_probs)  # prints: tensor([[0.0197, 0.9333, 0.0471]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mget_tokenizer(\u001b[39m'\u001b[39m\u001b[39mViT-B-32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39m# text = tokenizer([\"MRI\", \"Xray\", \"CT\"])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# tokenizer = AutoTokenizer.from_pretrained('roberta-base')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mdecode([ \u001b[39m49406\u001b[39m, \u001b[39m24773\u001b[39m, \u001b[39m49407\u001b[39m,     \u001b[39m0\u001b[39m,     \u001b[39m0\u001b[39m,])\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(token)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "# text = tokenizer([\"MRI\", \"Xray\", \"CT\"])\n",
    "# tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "token = tokenizer.decode([ 49406, 24773, 49407,     0,     0,])\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption prediction\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "#('coca_ViT-B-32', 'laion2b_s13b_b90k')\n",
    "# model, _, transform = open_clip.create_model_and_transforms(\n",
    "#     model_name=\"coca_ViT-B-32\",  #coca_ViT-L-14, \n",
    "#     pretrained=\"laion2b_s13b_b90k\", #mscoco_finetuned_laion2B-s13B-b90k\n",
    "# )\n",
    "# /mnt/eds_share/Users/yilu.zhou/Development/log/open_clip/2023_07_28-13_55_44-model_coca_ViT-L-14-lr_1e-05-b_32-j_4-p_amp/checkpoints/epoch_1.pt\n",
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=\"/mnt/eds_share/Users/yilu.zhou/Development/log/open_clip_GlobusSrgMapData_crop_square/2023_08_08-22_02_21-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_78.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0.dev20230816+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/coca_model.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  text = text[:, :-1] if embed_cls else text # make space for CLS token\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[255, 36, 64]' is invalid for input of size 195840",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mthoracic\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlumbar\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mthoracic lumbar\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m dummy_texts \u001b[39m=\u001b[39m tokenizer(sentences) \u001b[39m# , \"Xray\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, (dummy_image, dummy_texts), \u001b[39m\"\u001b[39;49m\u001b[39mclip_resnet.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, export_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     23\u001b[0m   input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mIMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     24\u001b[0m   output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_IMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_TEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     25\u001b[0m   opset_version\u001b[39m=\u001b[39;49m\u001b[39m14\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m   dynamic_axes\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     27\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mIMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m     28\u001b[0m           \u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mimage_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m       },\n\u001b[1;32m     30\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mTEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m     31\u001b[0m           \u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtext_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m       },\n\u001b[1;32m     33\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_IMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m     34\u001b[0m           \u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mimage_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m           \u001b[39m1\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtext_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     36\u001b[0m       },\n\u001b[1;32m     37\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_TEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m     38\u001b[0m           \u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtext_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m           \u001b[39m1\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mimage_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m       },\n\u001b[1;32m     41\u001b[0m   }\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[39m# image_features = model.encode_image(dummy_image)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# text_features = model.encode_text(dummy_texts)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39m#https://github.com/openai/CLIP/pull/219\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1582\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1580\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1582\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1583\u001b[0m     model,\n\u001b[1;32m   1584\u001b[0m     args,\n\u001b[1;32m   1585\u001b[0m     verbose,\n\u001b[1;32m   1586\u001b[0m     input_names,\n\u001b[1;32m   1587\u001b[0m     output_names,\n\u001b[1;32m   1588\u001b[0m     operator_export_type,\n\u001b[1;32m   1589\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1590\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1591\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1592\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1593\u001b[0m )\n\u001b[1;32m   1595\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1597\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1598\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:1287\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1286\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1287\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1288\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1289\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1290\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    134\u001b[0m     wrapper,\n\u001b[1;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/coca_model.py:158\u001b[0m, in \u001b[0;36mCoCa.forward\u001b[0;34m(self, image, text, embed_cls, image_latent, image_embs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m# TODO: add assertion to avoid bugs?\u001b[39;00m\n\u001b[1;32m    156\u001b[0m labels \u001b[39m=\u001b[39m text[:, \u001b[39m-\u001b[39mtoken_embs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[0;32m--> 158\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_decoder(image_embs, token_embs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    160\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimage_features\u001b[39m\u001b[39m\"\u001b[39m: image_latent,\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m\"\u001b[39m: text_latent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlogit_scale\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_scale\u001b[39m.\u001b[39mexp()\n\u001b[1;32m    165\u001b[0m }\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:714\u001b[0m, in \u001b[0;36mMultimodalTransformer.forward\u001b[0;34m(self, image_embs, text_embs)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m         text_embs \u001b[39m=\u001b[39m resblock(text_embs, attn_mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask[:seq_len, :seq_len])\n\u001b[0;32m--> 714\u001b[0m         text_embs \u001b[39m=\u001b[39m cross_attn(text_embs, k_x\u001b[39m=\u001b[39;49mimage_embs, v_x\u001b[39m=\u001b[39;49mimage_embs)\n\u001b[1;32m    716\u001b[0m x \u001b[39m=\u001b[39m text_embs\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    717\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:242\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m k_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(k_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m k_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q_x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(q_x), k_x\u001b[39m=\u001b[39;49mk_x, v_x\u001b[39m=\u001b[39;49mv_x, attn_mask\u001b[39m=\u001b[39;49mattn_mask))\n\u001b[1;32m    243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:228\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m v_x \u001b[39m=\u001b[39m v_x \u001b[39mif\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m q_x\n\u001b[1;32m    227\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(q_x\u001b[39m.\u001b[39mdtype) \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    229\u001b[0m     q_x, k_x, v_x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m    230\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/activation.py:1237\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1224\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1225\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1235\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1237\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1238\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1239\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1240\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1241\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1242\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1243\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1244\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1245\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1246\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1247\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1249\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/functional.py:5348\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5346\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(tgt_len, bsz \u001b[39m*\u001b[39m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5347\u001b[0m \u001b[39mif\u001b[39;00m static_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 5348\u001b[0m     k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49mview(k\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], bsz \u001b[39m*\u001b[39;49m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5350\u001b[0m     \u001b[39m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5351\u001b[0m     \u001b[39massert\u001b[39;00m static_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m bsz \u001b[39m*\u001b[39m num_heads, \\\n\u001b[1;32m   5352\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting static_k.size(0) of \u001b[39m\u001b[39m{\u001b[39;00mbsz\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39mnum_heads\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mstatic_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[255, 36, 64]' is invalid for input of size 195840"
     ]
    }
   ],
   "source": [
    "#classification for ONNX\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "\n",
    "# /mnt/eds_share/Users/yilu.zhou/Development/log/open_clip/2023_07_28-13_55_44-model_coca_ViT-L-14-lr_1e-05-b_32-j_4-p_amp/checkpoints/epoch_3.pt\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=\"/mnt/eds_share/Users/yilu.zhou/Development/log/open_clip_GlobusSrgMapData_crop_square/2023_08_08-22_02_21-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_78.pt\"\n",
    ")\n",
    "model = model.eval()\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "dummy_image = preprocess(im).unsqueeze(0)\n",
    "sentences = [\"thoracic\", \"lumbar\", \"thoracic lumbar\"]\n",
    "dummy_texts = tokenizer(sentences) # , \"Xray\"\n",
    "\n",
    "torch.onnx.export(model, (dummy_image, dummy_texts), \"clip_resnet.onnx\", export_params=True,\n",
    "  input_names=[\"IMAGE\", \"TEXT\"],\n",
    "  output_names=[\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"],\n",
    "  opset_version=14,\n",
    "  dynamic_axes={\n",
    "      \"IMAGE\": {\n",
    "          0: \"image_batch_size\",\n",
    "      },\n",
    "      \"TEXT\": {\n",
    "          0: \"text_batch_size\",\n",
    "      },\n",
    "      \"LOGITS_PER_IMAGE\": {\n",
    "          0: \"image_batch_size\",\n",
    "          1: \"text_batch_size\",\n",
    "      },\n",
    "      \"LOGITS_PER_TEXT\": {\n",
    "          0: \"text_batch_size\",\n",
    "          1: \"image_batch_size\",\n",
    "      },\n",
    "  }\n",
    ")\n",
    "\n",
    "# image_features = model.encode_image(dummy_image)\n",
    "# text_features = model.encode_text(dummy_texts)\n",
    "\n",
    "# image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "# text_probs = text_probs.cpu().tolist()[0]\n",
    "\n",
    "# # Construct the dictionary\n",
    "# class_predict_dict = dict(zip(sentences, text_probs))\n",
    "# # Extract the key with the largest value\n",
    "# class_predict = max(class_predict_dict, key=class_predict_dict.get)\n",
    "# print(f\"class_predict: {class_predict}\")  # prints: [[1., 0., 0.]]\n",
    "\n",
    "#https://github.com/openai/CLIP/pull/219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[255, 36, 64]' is invalid for input of size 195840",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m ONNX_FILE_PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mopenclip.onnx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39m# torch.onnx.export(model, (dummy_image, dummy_texts), ONNX_FILE_PATH, input_names=[\"IMAGE\", \"TEXT\"],\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#                   output_names=[\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"], export_params=True)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, (dummy_image, dummy_texts), ONNX_FILE_PATH, input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mIMAGE\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     23\u001b[0m                   output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mLOGITS_PER_TEXT\u001b[39;49m\u001b[39m\"\u001b[39;49m], export_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1582\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1580\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1582\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1583\u001b[0m     model,\n\u001b[1;32m   1584\u001b[0m     args,\n\u001b[1;32m   1585\u001b[0m     verbose,\n\u001b[1;32m   1586\u001b[0m     input_names,\n\u001b[1;32m   1587\u001b[0m     output_names,\n\u001b[1;32m   1588\u001b[0m     operator_export_type,\n\u001b[1;32m   1589\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1590\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1591\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1592\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1593\u001b[0m )\n\u001b[1;32m   1595\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1597\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1598\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:1287\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1286\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1287\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1288\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1289\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1290\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    134\u001b[0m     wrapper,\n\u001b[1;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/jit/_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/coca_model.py:158\u001b[0m, in \u001b[0;36mCoCa.forward\u001b[0;34m(self, image, text, embed_cls, image_latent, image_embs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m# TODO: add assertion to avoid bugs?\u001b[39;00m\n\u001b[1;32m    156\u001b[0m labels \u001b[39m=\u001b[39m text[:, \u001b[39m-\u001b[39mtoken_embs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[0;32m--> 158\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_decoder(image_embs, token_embs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    160\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimage_features\u001b[39m\u001b[39m\"\u001b[39m: image_latent,\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m\"\u001b[39m: text_latent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlogit_scale\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_scale\u001b[39m.\u001b[39mexp()\n\u001b[1;32m    165\u001b[0m }\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:714\u001b[0m, in \u001b[0;36mMultimodalTransformer.forward\u001b[0;34m(self, image_embs, text_embs)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m         text_embs \u001b[39m=\u001b[39m resblock(text_embs, attn_mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask[:seq_len, :seq_len])\n\u001b[0;32m--> 714\u001b[0m         text_embs \u001b[39m=\u001b[39m cross_attn(text_embs, k_x\u001b[39m=\u001b[39;49mimage_embs, v_x\u001b[39m=\u001b[39;49mimage_embs)\n\u001b[1;32m    716\u001b[0m x \u001b[39m=\u001b[39m text_embs\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    717\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:242\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m k_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(k_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m k_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q_x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(q_x), k_x\u001b[39m=\u001b[39;49mk_x, v_x\u001b[39m=\u001b[39;49mv_x, attn_mask\u001b[39m=\u001b[39;49mattn_mask))\n\u001b[1;32m    243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/open_clip/transformer.py:228\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m v_x \u001b[39m=\u001b[39m v_x \u001b[39mif\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m q_x\n\u001b[1;32m    227\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(q_x\u001b[39m.\u001b[39mdtype) \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    229\u001b[0m     q_x, k_x, v_x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m    230\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/activation.py:1237\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1224\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1225\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1235\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1237\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1238\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1239\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1240\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1241\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1242\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1243\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1244\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1245\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1246\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1247\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1249\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/functional.py:5348\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5346\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(tgt_len, bsz \u001b[39m*\u001b[39m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5347\u001b[0m \u001b[39mif\u001b[39;00m static_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 5348\u001b[0m     k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49mview(k\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], bsz \u001b[39m*\u001b[39;49m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5350\u001b[0m     \u001b[39m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5351\u001b[0m     \u001b[39massert\u001b[39;00m static_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m bsz \u001b[39m*\u001b[39m num_heads, \\\n\u001b[1;32m   5352\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting static_k.size(0) of \u001b[39m\u001b[39m{\u001b[39;00mbsz\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39mnum_heads\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mstatic_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[255, 36, 64]' is invalid for input of size 195840"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from albumentations import Resize, Compose\n",
    "from albumentations.pytorch.transforms import  ToTensor\n",
    "from albumentations.augmentations.transforms import Normalize\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "sentences = [\"thoracic\", \"lumbar\", \"thoracic lumbar\"]\n",
    "dummy_texts = tokenizer(sentences) \n",
    "\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "dummy_image = preprocess(im).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "ONNX_FILE_PATH = 'openclip.onnx'\n",
    "# torch.onnx.export(model, (dummy_image, dummy_texts), ONNX_FILE_PATH, input_names=[\"IMAGE\", \"TEXT\"],\n",
    "#                   output_names=[\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model, (dummy_image, dummy_texts), ONNX_FILE_PATH, input_names=[\"IMAGE\", \"TEXT\"],\n",
    "                  output_names=[\"LOGITS_PER_TEXT\"], export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anteroposterior thoracic lumbar \n"
     ]
    }
   ],
   "source": [
    "# caption prediction\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "im = transform(im).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    generated = model.generate(im)\n",
    "\n",
    "print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard CLIP model, image, text on cpu\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "# onnx cannot work with cuda\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"coca_ViT-L-14\",\n",
    "    pretrained=\"/mnt/eds_share/Users/yilu.zhou/Development/log/open_clip_GlobusSrgMapData_crop_square/2023_08_08-22_02_21-model_coca_ViT-L-14-lr_1e-06-b_32-j_4-p_amp/checkpoints/epoch_78.pt\"\n",
    ")\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
    "\n",
    "# batch first\n",
    "image_path = '/mnt/eds_share/share/Spine2D/GlobusSrgMapData_crop_square/test/images/anon_d4730414.dcm_lateral_full_lumbar_thoracic.jpg'\n",
    "im = Image.open(image_path).convert(\"RGB\")\n",
    "im = im.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "image = preprocess(im).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
    "# image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
    "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# batch first\n",
    "tokenizer = open_clip.get_tokenizer('coca_ViT-L-14')\n",
    "sentences = [\"thoracic\", \"lumbar\", \"thoracic lumbar\"]\n",
    "text = tokenizer(sentences).cpu()\n",
    "# text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).cpu() # [3, 77]\n",
    "text_onnx = text.detach().cpu().numpy().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/97262\n",
    "import onnxscript\n",
    "\n",
    "# Assuming you use opset17\n",
    "from onnxscript.onnx_opset import opset13 as op\n",
    "\n",
    "custom_opset = onnxscript.values.Opset(domain=\"torch.onnx\", version=1)\n",
    "\n",
    "\n",
    "@onnxscript.script(custom_opset)\n",
    "def ScaledDotProductAttention(\n",
    "    query,\n",
    "    key,\n",
    "    value,\n",
    "    dropout_p,\n",
    "):\n",
    "    # Swap the last two axes of key\n",
    "    key_shape = op.Shape(key)\n",
    "    key_last_dim = key_shape[-1:]\n",
    "    key_second_last_dim = key_shape[-2:-1]\n",
    "    key_first_dims = key_shape[:-2]\n",
    "    # Contract the dimensions that are not the last two so we can transpose\n",
    "    # with a static permutation.\n",
    "    key_squeezed_shape = op.Concat(\n",
    "        op.Constant(value_ints=[-1]), key_second_last_dim, key_last_dim, axis=0\n",
    "    )\n",
    "    key_squeezed = op.Reshape(key, key_squeezed_shape)\n",
    "    key_squeezed_transposed = op.Transpose(key_squeezed, perm=[0, 2, 1])\n",
    "    key_transposed_shape = op.Concat(key_first_dims, key_last_dim, key_second_last_dim, axis=0)\n",
    "    key_transposed = op.Reshape(key_squeezed_transposed, key_transposed_shape)\n",
    "\n",
    "    embedding_size = op.CastLike(op.Shape(query)[-1], query)\n",
    "    scale = op.Div(1.0, op.Sqrt(embedding_size))\n",
    "\n",
    "    # https://github.com/pytorch/pytorch/blob/12da0c70378b5be9135c6fda62a9863bce4a4818/aten/src/ATen/native/transformers/attention.cpp#L653\n",
    "    # Scale q, k before matmul for stability see https://tinyurl.com/sudb9s96 for math\n",
    "    query_scaled = op.Mul(query, op.Sqrt(scale))\n",
    "    key_transposed_scaled = op.Mul(key_transposed, op.Sqrt(scale))\n",
    "    attn_weight = op.Softmax(\n",
    "        op.MatMul(query_scaled, key_transposed_scaled),\n",
    "        axis=-1,\n",
    "    )\n",
    "    attn_weight, _ = op.Dropout(attn_weight, dropout_p)\n",
    "    return op.MatMul(attn_weight, value)\n",
    "\n",
    "\n",
    "def custom_scaled_dot_product_attention(g, query, key, value, attn_mask, dropout, is_causal, scale=None):\n",
    "    return g.onnxscript_op(ScaledDotProductAttention, query, key, value, dropout).setType(value.type())\n",
    "\n",
    "\n",
    "torch.onnx.register_custom_op_symbolic(\n",
    "    symbolic_name=\"aten::scaled_dot_product_attention\",\n",
    "    symbolic_fn=custom_scaled_dot_product_attention,\n",
    "    opset_version=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLIP ONNX] Start convert visual model\n",
      "[CLIP ONNX] Start check visual model\n",
      "[CLIP ONNX] Start convert textual model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CoCa' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m textual_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclip_textual.onnx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m onnx_model \u001b[39m=\u001b[39m clip_onnx(model, visual_path\u001b[39m=\u001b[39mvisual_path, textual_path\u001b[39m=\u001b[39mtextual_path)\n\u001b[0;32m----> 8\u001b[0m onnx_model\u001b[39m.\u001b[39;49mconvert2onnx(image, text, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\u001b[39;00m\n\u001b[1;32m     10\u001b[0m onnx_model\u001b[39m.\u001b[39mstart_sessions(providers\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCPUExecutionProvider\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# cpu mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/clip_converter.py:88\u001b[0m, in \u001b[0;36mclip_converter.convert2onnx\u001b[0;34m(self, visual_input, textual_input, verbose, visual_wrapper, textual_wrapper, visual_export_params, textual_export_params)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     87\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[CLIP ONNX] Start convert textual model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_textual(textual_input, textual_wrapper, textual_export_params)\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     90\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[CLIP ONNX] Start check textual model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/clip_converter.py:55\u001b[0m, in \u001b[0;36mclip_converter.convert_textual\u001b[0;34m(self, dummy_input, wrapper, export_params)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_textual\u001b[39m(\u001b[39mself\u001b[39m, dummy_input, wrapper\u001b[39m=\u001b[39mTextual,\n\u001b[1;32m     54\u001b[0m                     export_params\u001b[39m=\u001b[39mDEFAULT_EXPORT):\n\u001b[0;32m---> 55\u001b[0m     textual \u001b[39m=\u001b[39m wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[1;32m     56\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtorch_export(textual, dummy_input, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtextual_path,\n\u001b[1;32m     57\u001b[0m                       export_params\u001b[39m=\u001b[39mexport_params)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monnx_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtextual_path)\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/utils.py:9\u001b[0m, in \u001b[0;36mTextual.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[1;32m      8\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtransformer\n\u001b[1;32m     10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpositional_embedding\n\u001b[1;32m     11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransformer\n",
      "File \u001b[0;32m/opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CoCa' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "# Create CLIP-ONNX object to convert model to onnx\n",
    "from clip_onnx import clip_onnx\n",
    "\n",
    "visual_path = \"clip_visual.onnx\"\n",
    "textual_path = \"clip_textual.onnx\"\n",
    "\n",
    "onnx_model = clip_onnx(model, visual_path=visual_path, textual_path=textual_path)\n",
    "onnx_model.convert2onnx(image, text, verbose=True)\n",
    "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode\n",
    "\n",
    "#Use for standard CLIP API. Batch inference\n",
    "image_features = onnx_model.encode_image(image_onnx)\n",
    "text_features = onnx_model.encode_text(text_onnx)\n",
    "\n",
    "logits_per_image, logits_per_text = onnx_model(image_onnx, text_onnx)\n",
    "probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421067 0.00299571]]\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/96944\n",
    "\n",
    "# /opt/miniconda/envs/yz_openclip/lib/python3.10/site-packages/clip_onnx/utils.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yz_openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
